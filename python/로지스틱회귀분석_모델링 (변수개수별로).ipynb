{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "\n",
    "train_over = pd.read_csv('train_over_A.csv',encoding='CP949')\n",
    "#train_over = pd.read_csv('train_over_B.csv',encoding='CP949')\n",
    "train_over = train_over.iloc[:,1:]\n",
    "\n",
    "test_over = pd.read_csv('test_over_A.csv',encoding='CP949')\n",
    "test_over = test_over.iloc[:,1:]\n",
    "train_over.shape, test_over.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_over = train_over.loc[:,['dv1','dv2','dv4','dv3','dv15','CITY','dv17',\n",
    "                            'M_WK_TIME','dv12','dv8','dv5','dv11','dv7',\n",
    "                            'HW_S_R','dv14','dv20','M_SUI_CON']]\n",
    "\n",
    "test_over = test_over.loc[:,['dv1','dv2','dv4','dv3','dv15','CITY','dv17',\n",
    "                            'M_WK_TIME','dv12','dv8','dv5','dv11','dv7',\n",
    "                            'HW_S_R','dv14','dv20','M_SUI_CON']]\n",
    "\n",
    "\n",
    "train_over.shape, test_over.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시도[CITY]\n",
    "## 각 도시들을 1~17로 MAPPING \n",
    "print(train_over.loc[:,'CITY'].value_counts())\n",
    "train_over[['CITY']] = train_over[['CITY']].replace({'경기':1, \n",
    "                                     '서울':2,\n",
    "                                     '부산':3,\n",
    "                                     '경남':4,\n",
    "                                     '인천':5,\n",
    "                                     '경북':6,\n",
    "                                     '대구':7,\n",
    "                                     '충남':8,\n",
    "                                     '전남':9,\n",
    "                                     '전북':10,\n",
    "                                     '광주':11,\n",
    "                                     '충북':12,\n",
    "                                     '대전':13,\n",
    "                                     '강원':14,\n",
    "                                     '울산':15,\n",
    "                                     '제주':16,\n",
    "                                     '세종':17})\n",
    "print(train_over.loc[:,'CITY'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시도[CITY]\n",
    "## 각 도시들을 1~17로 MAPPING \n",
    "print(test_over.loc[:,'CITY'].value_counts())\n",
    "test_over[['CITY']] = test_over[['CITY']].replace({'경기':1, \n",
    "                                     '서울':2,\n",
    "                                     '부산':3,\n",
    "                                     '경남':4,\n",
    "                                     '인천':5,\n",
    "                                     '경북':6,\n",
    "                                     '대구':7,\n",
    "                                     '충남':8,\n",
    "                                     '전남':9,\n",
    "                                     '전북':10,\n",
    "                                     '광주':11,\n",
    "                                     '충북':12,\n",
    "                                     '대전':13,\n",
    "                                     '강원':14,\n",
    "                                     '울산':15,\n",
    "                                     '제주':16,\n",
    "                                     '세종':17})\n",
    "print(test_over.loc[:,'CITY'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 팩터화\n",
    "train_over['dv1'] = train_over['dv1'].astype('category')\n",
    "train_over['dv2'] = train_over['dv2'].astype('category')\n",
    "train_over['dv4'] = train_over['dv4'].astype('category')\n",
    "train_over['dv3'] = train_over['dv3'].astype('category')\n",
    "train_over['dv15'] = train_over['dv15'].astype('category')\n",
    "train_over['CITY'] = train_over['CITY'].astype('category')\n",
    "train_over['dv17'] = train_over['dv17'].astype('category')\n",
    "#train_over['M_WK_TIME'] = train_over['M_WK_TIME'].astype('category')\n",
    "train_over['dv12'] = train_over['dv12'].astype('category')\n",
    "train_over['dv8'] = train_over['dv8'].astype('category')\n",
    "train_over['dv5'] = train_over['dv5'].astype('category')\n",
    "train_over['dv11'] = train_over['dv11'].astype('category')\n",
    "train_over['dv7'] = train_over['dv7'].astype('category')\n",
    "train_over['HW_S_R'] = train_over['HW_S_R'].astype('category')\n",
    "train_over['dv14'] = train_over['dv14'].astype('category')\n",
    "train_over['dv20'] = train_over['dv20'].astype('category')\n",
    "train_over['M_SUI_CON'] = train_over['M_SUI_CON'].astype('category')\n",
    "train_over.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 데이터 팩터화\n",
    "test_over['dv1'] = test_over['dv1'].astype('category')\n",
    "test_over['dv2'] = test_over['dv2'].astype('category')\n",
    "test_over['dv4'] = test_over['dv4'].astype('category')\n",
    "test_over['dv3'] = test_over['dv3'].astype('category')\n",
    "test_over['dv15'] = test_over['dv15'].astype('category')\n",
    "test_over['CITY'] = test_over['CITY'].astype('category')\n",
    "test_over['dv17'] = test_over['dv17'].astype('category')\n",
    "#test_over['M_WK_TIME'] = test_over['M_WK_TIME'].astype('category')\n",
    "test_over['dv12'] = test_over['dv12'].astype('category')\n",
    "test_over['dv8'] = test_over['dv8'].astype('category')\n",
    "test_over['dv5'] = test_over['dv5'].astype('category')\n",
    "test_over['dv11'] = test_over['dv11'].astype('category')\n",
    "test_over['dv7'] = test_over['dv7'].astype('category')\n",
    "test_over['HW_S_R'] = test_over['HW_S_R'].astype('category')\n",
    "test_over['dv14'] = test_over['dv14'].astype('category')\n",
    "test_over['dv20'] = test_over['dv20'].astype('category')\n",
    "test_over['M_SUI_CON'] = test_over['M_SUI_CON'].astype('category')\n",
    "test_over.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 있는지 확인 --> 결측치 없음\n",
    "print(train_over.shape, test_over.shape)\n",
    "train_over = train_over.dropna(axis=0)\n",
    "test_over = test_over.dropna(axis=0)\n",
    "print(train_over.shape, test_over.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Train과 Test는 기존 R로 분할을 했었기에\n",
    "X와 Y에 대해서만 분리를 진행한다.\n",
    "'''\n",
    "\n",
    "# Train_over\n",
    "#feature_columns = list(train_over.columns.difference(['M_SUI_CON']))\n",
    "feature_columns = list(train_over.columns[:-1])\n",
    "train_x = train_over[feature_columns]\n",
    "train_y = train_over[['M_SUI_CON']]\n",
    "test_x = test_over[feature_columns]\n",
    "test_y = test_over[['M_SUI_CON']]\n",
    "\n",
    "\n",
    "\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 지표 구하는 함수 구현\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def model_evaluation(label, predict):\n",
    "    cf_matrix = confusion_matrix(label, predict)\n",
    "    Accuracy = (cf_matrix[0][0] + cf_matrix[1][1]) / sum(sum(cf_matrix))\n",
    "    Precision = cf_matrix[1][1] / (cf_matrix[1][1] + cf_matrix[0][1])\n",
    "    Recall = cf_matrix[1][1] / (cf_matrix[1][1] + cf_matrix[1][0])\n",
    "    Specificity = cf_matrix[0][0] / (cf_matrix[0][0] + cf_matrix[0][1])\n",
    "    F1_Score = (2 * Recall * Precision) / (Recall + Precision)\n",
    "    \n",
    "    return Accuracy, Precision, Recall, Specificity, F1_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "performance_df = pd.DataFrame(columns=['feature_num','Accuracy', 'Precision', 'Recall', 'Specificity', 'F1_Score'])\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0, len(feature_columns),1):\n",
    "\n",
    "    train_x_final2 = train_x.loc[:,feature_columns]\n",
    "    train_y_final2 = train_y\n",
    "    test_x_final2 = test_x.loc[:,feature_columns]\n",
    "    test_y_final2 = test_y\n",
    "    #print(train_x_final2.shape, test_x_final2.shape)\n",
    "\n",
    "    log_reg2 = LogisticRegression()\n",
    "    log_reg2.fit(train_x_final2, train_y_final2)\n",
    "\n",
    "    pred_y_final2 = log_reg2.predict(test_x_final2) #test_data넣어보기\n",
    "    pred_y_final2\n",
    "    \n",
    "    print(confusion_matrix(test_y_final2, pred_y_final2))\n",
    "    \n",
    "    feature_num_tmp = len(feature_columns)\n",
    "    accuracy_tmp = model_evaluation(test_y_final2, pred_y_final2)[0]\n",
    "    precision_tmp = model_evaluation(test_y_final2, pred_y_final2)[1]\n",
    "    recall_tmp = model_evaluation(test_y_final2, pred_y_final2)[2]\n",
    "    specificity_tmp = model_evaluation(test_y_final2, pred_y_final2)[3]\n",
    "    f1score_tmp = model_evaluation(test_y_final2, pred_y_final2)[4]\n",
    "    \n",
    "    performance_df = performance_df.append(\n",
    "        pd.DataFrame([[feature_num_tmp, accuracy_tmp, precision_tmp, recall_tmp, specificity_tmp, f1score_tmp]], \n",
    "                   columns=['feature_num','Accuracy', 'Precision', 'Recall', 'Specificity', 'F1_Score']), ignore_index=True)\n",
    "\n",
    "\n",
    "    feature_columns.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df_sorting = performance_df.sort_values(by='feature_num')\n",
    "performance_df_sorting.reset_index(drop=True, inplace=True)\n",
    "performance_df_sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
